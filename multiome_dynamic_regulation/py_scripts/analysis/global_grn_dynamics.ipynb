{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import dictys\n",
    "from dictys.net import stat\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_custom import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file paths \n",
    "output_folder = '/ocean/projects/cis240075p/asachan/datasets/B_Cell/multiome_1st_donor_UPMC_aggr/dictys_outs/actb1_added_v2/output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "dictys_dynamic_object = dictys.net.dynamic_network.from_file(os.path.join(output_folder, 'dynamic.h5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rank TFs unbiasedly based on their expression and regulation curve chars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the top genes of TF subnetworks based on the beta curve chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[225, 305, 25, 150, 243, 251, 299, 303, 68, 36, 48, 133, 46, 118, 134]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load ranked TFs list \n",
    "ranked_tfs_pb = pd.read_csv(os.path.join(output_folder, 'ranked_tfs_pb.csv'))\n",
    "pb_top_tfs = ranked_tfs_pb[0:15]\n",
    "#convert to list of TF_name\n",
    "pb_top_tfs_list = pb_top_tfs['TF_name'].tolist()\n",
    "# get tf_indices from pb_top_tfs_list\n",
    "tf_indices_top_pb_tfs, _, _ = get_tf_indices(dictys_dynamic_object, pb_top_tfs_list)\n",
    "display(tf_indices_top_pb_tfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the edge strengths of the top tfs with all relevant genes (dropping genes based on sparsity)\n",
    "pts, fsmooth = dictys_dynamic_object.linspace(0,2,100,0.0005)\n",
    "stat1_net = fsmooth(stat.net(dictys_dynamic_object))\n",
    "stat1_netbin = stat.fbinarize(stat1_net,sparsity=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the binary network to get the indices of genes to keep based on sparsity\n",
    "dnetbin = stat1_netbin.compute(pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(551, 11907, 100)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(dnetbin.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 11907, 100)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "subnetworks = dnetbin[np.ix_(tf_indices_top_pb_tfs, range(dnetbin.shape[1]), range(dnetbin.shape[2]))]\n",
    "display(subnetworks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "647\n"
     ]
    }
   ],
   "source": [
    "# Calculate sparsity for each gene (across all TFs and timepoints)\n",
    "# Mean across TFs (axis 0) and time (axis 2)\n",
    "gene_density = (subnetworks != 0).mean(axis=(0, 2))  # Shape: (11907,)\n",
    "# Keep genes that are non-zero more than 20% of the time\n",
    "genes_to_keep = gene_density > 0.05\n",
    "genes_to_keep_indices = np.where(genes_to_keep)[0]\n",
    "# Create reverse mapping: index -> gene_name\n",
    "ndict = dictys_dynamic_object.ndict\n",
    "index_to_gene = {idx: name for name, idx in ndict.items()}\n",
    "# Get gene names for kept indices directly\n",
    "kept_gene_names = [index_to_gene[idx] for idx in genes_to_keep_indices]\n",
    "print(len(genes_to_keep_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15, 647, 100)\n"
     ]
    }
   ],
   "source": [
    "# Filter the subnetworks array to keep only non-sparse genes\n",
    "filtered_subnetworks = subnetworks[:, genes_to_keep_indices, :]\n",
    "print(filtered_subnetworks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from multiprocessing import Pool\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "def process_chunk(args):\n",
    "    \"\"\"Process a chunk of curves to compute characteristics\"\"\"\n",
    "    chunk_df, dtime = args\n",
    "    results = {}\n",
    "    for idx in chunk_df.index:\n",
    "        curve = chunk_df.loc[idx]\n",
    "        char_dict = compute_curve_characteristics(pd.DataFrame(curve).T, dtime)\n",
    "        results[idx] = char_dict.iloc[0]\n",
    "    return pd.DataFrame.from_dict(results, orient='index')\n",
    "\n",
    "# Main processing code\n",
    "dtime = pd.Series(np.linspace(0, 1, betas_dcurve.shape[1]))\n",
    "\n",
    "# Split data into chunks\n",
    "n_cores = 8  # Adjust based on your CPU\n",
    "chunk_size = math.ceil(len(betas_dcurve) / n_cores)\n",
    "chunks = []\n",
    "\n",
    "for i in range(0, len(betas_dcurve), chunk_size):\n",
    "    chunk = betas_dcurve.iloc[i:i + chunk_size]\n",
    "    chunks.append((chunk, dtime))\n",
    "\n",
    "# Process chunks in parallel with progress bar\n",
    "with Pool(n_cores) as pool:\n",
    "    results = list(tqdm(\n",
    "        pool.imap(process_chunk, chunks),\n",
    "        total=len(chunks),\n",
    "        desc=\"Processing curves\"\n",
    "    ))\n",
    "\n",
    "# Combine results and sort\n",
    "final_dchar = pd.concat(results).sort_index()\n",
    "# final_dchar.to_csv(os.path.join(output_folder, 'betas_chars_ntfs_ngenes.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_curves_by_category(dchar, ntops=(20,20,20,30)):\n",
    "    \"\"\"\n",
    "    Get top curves for different patterns based on Terminal and Transient logFC.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    dchar : pd.DataFrame\n",
    "        DataFrame with curve characteristics, having multi-index (TF, Target)\n",
    "        and columns ['Terminal logFC', 'Transient logFC', 'Switching time']\n",
    "    ntops : tuple\n",
    "        Number of top curves to return for (activating, inactivating, transient_up, transient_down)\n",
    "    \"\"\"\n",
    "    categories = {}\n",
    "    \n",
    "    # Activating (positive Terminal logFC)\n",
    "    t1 = dchar.sort_values('Terminal logFC', ascending=False).head(ntops[0])\n",
    "    categories['activating'] = t1.sort_values('Switching time')\n",
    "    \n",
    "    # Inactivating (negative Terminal logFC)\n",
    "    t1 = dchar.sort_values('Terminal logFC', ascending=True).head(ntops[1])\n",
    "    categories['inactivating'] = t1.sort_values('Switching time')\n",
    "    \n",
    "    # Transient up\n",
    "    t1 = dchar.sort_values('Transient logFC', ascending=False).head(ntops[2])\n",
    "    categories['transient_up'] = t1\n",
    "    \n",
    "    # Transient down\n",
    "    t1 = dchar.sort_values('Transient logFC', ascending=True).head(ntops[3])\n",
    "    categories['transient_down'] = t1\n",
    "    \n",
    "    # Extract TF-target pairs for each category\n",
    "    activating_pairs = [(idx[0], idx[1]) for idx in categories['activating'].index]\n",
    "    inactivating_pairs = [(idx[0], idx[1]) for idx in categories['inactivating'].index]\n",
    "    transient_up_pairs = [(idx[0], idx[1]) for idx in categories['transient_up'].index]\n",
    "    transient_down_pairs = [(idx[0], idx[1]) for idx in categories['transient_down'].index]\n",
    "    \n",
    "    return {\n",
    "        'activating': activating_pairs,\n",
    "        'inactivating': inactivating_pairs,\n",
    "        'transient_up': transient_up_pairs,\n",
    "        'transient_down': transient_down_pairs\n",
    "    }\n",
    "\n",
    "# Example usage:\n",
    "sorted_categories = get_top_curves_by_category(final_dchar, ntops=(20,20,30,30))\n",
    "\n",
    "# Print results\n",
    "for category, pairs in sorted_categories.items():\n",
    "    print(f\"\\nTop pairs for {category}:\")\n",
    "    for tf, target in pairs:\n",
    "        print(f\"TF: {tf}, Target: {target}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dictys",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
